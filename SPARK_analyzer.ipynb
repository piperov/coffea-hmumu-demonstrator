{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img  src=\"https://swan.web.cern.ch/sites/swan.web.cern.ch/files/pictures/logo_swan_letters.png\" alt=\"SWAN\" style=\"float: left; width: 15%; margin-right: 5%; margin-left: 17%; margin-top: 1.0em; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://spark.apache.org/images/spark-logo-trademark.png\" alt=\"EP-SFT\" style=\"float: left; width: 25%; margin-right: 0%; margin-left: 0%; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://cms-docdb.cern.ch/cgi-bin/PublicDocDB/RetrieveFile?docid=3045&filename=CMSlogo_color_label_1024_May2014.png&version=3\" alt=\"CMS\" style=\"float: left; width: 12%; margin-left: 5%; margin-right: 5%; margin-bottom: 2.0em;\"> -->\n",
    "<p style=\"clear: both;\">\n",
    "<div style=\"text-align:center\"><h1>CMS H&#8594;µµ analysis  \n",
    "     <br> with Coffea package from Fermilab</h1></div>\n",
    "<div style=\"text-align:center\"><i>Author: Dmitry Kondratyev, based on example code by Lindsey Gray and adapted for the demonstration by Stefan Piperov</i></div>\n",
    "<hr style=\"border-top-width: 4px; border-top-color: #34609b;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for Higgs boson decaying into two muons\n",
    "\n",
    "This code uses awkward array toolset, and utilizing Coffea [histograms](https://coffeateam.github.io/coffea/modules/coffea.hist.html).\n",
    "This also shows the analysis object syntax implemented by Coffea [JaggedCandidateArray](https://coffeateam.github.io/coffea/api/coffea.analysis_objects.JaggedCandidateMethods.html), and the usage of custom [accumulators](https://coffeateam.github.io/coffea/api/coffea.processor.AccumulatorABC.html) other than histograms.  Further, it introduces the [processor](https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html) concept and the interface to apache spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions to run on Hammer at Purdue:\n",
    "===\n",
    "It is assumed that this demonstration is being run in a Jupyter Lab session on Hammer, as described in the README file.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -l\n",
    "hostname\n",
    "ml anaconda/5.3.1-py37\n",
    "source activate hmumu_coffea\n",
    "echo $SPARK_HOME\n",
    "pwd\n",
    "voms-proxy-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare directories for output files  \n",
    "\n",
    "**Note** these paths need to be changed also further down in \"Out_dir\" in the \"Run the SPARK Executor\" cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mkdir -p /tmp/spiperov/hmm/coffea/test_2016_test/unbinned\n",
    "mkdir -p /depot/cms/hmm/coffea/Stefan/test_2018_test/unbinned\n",
    "mkdir -p /depot/cms/hmm/coffea/Stefan/test_2018_test/binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import coffea\n",
    "print(\"Coffea version: \", coffea.__version__)\n",
    "import socket\n",
    "\n",
    "print(socket.gethostname())\n",
    "from coffea import util\n",
    "import coffea.processor as processor\n",
    "import multiprocessing as mp\n",
    "print(f\"{mp.cpu_count()} CPUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data samples\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.samples_info import SamplesInfo\n",
    "samples = [\n",
    "### Data ###\n",
    "    'data_A', #not available for 2016\n",
    "    'data_B',\n",
    "    'data_C',\n",
    "    'data_D','data_E',\n",
    "    'data_F',\n",
    "    'data_G','data_H',\n",
    "\n",
    "### Essential MC ###    \n",
    "    'dy_m105_160_amc', \n",
    "    'dy_m105_160_vbf_amc',\n",
    "#      'ggh_amcPS', \n",
    "#     'vbf_powhegPS', \n",
    "#     'ttjets_dl',\n",
    "#    \"ewk_lljj_mll105_160_ptj0\",\n",
    "\n",
    "### Non-essential MC ### \n",
    "#     'ttjets_sl',\n",
    "#     'ttz',\n",
    "#     'ttw',\n",
    "#     'st_tw_top','st_tw_antitop',\n",
    "#     'ww_2l2nu',\n",
    "#     'wz_2l2q',\n",
    "#     'wz_3lnu',\n",
    "#     'wz_1l1nu2q',\n",
    "#      'zz',\n",
    "# # ##\n",
    "    \n",
    "]\n",
    "\n",
    "purdue = 'root://xrootd.rcac.purdue.edu/'\n",
    "\n",
    "year = '2017'\n",
    "label = 'test' # change this to save to other directory\n",
    "\n",
    "samp_info = SamplesInfo(year=year, out_path=f'test_{year}_{label}', server=purdue, debug=True)\n",
    "\n",
    "# 'outer' refers to parallelization by sample, 'inner' - by ROOT file in each sample\n",
    "samp_info.load(samples, nchunks=1, parallelize_outer=1, parallelize_inner=10)\n",
    "samp_info.compute_lumi_weights()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare SPARK executor\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell before establishing spark connection\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = os.environ['PYTHONPATH'] + ':' + '/usr/local/lib/python3.6/site-packages'\n",
    "# os.environ['PYTHONPATH'] = os.environ['PYTHONPATH'] + ':' + '/home/spiperov/dmitry/hmumu-coffea_20May2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x2b76fc4b8510>\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql\n",
    "from pyarrow.compat import guid\n",
    "from coffea.processor.spark.detail import _spark_initialize, _spark_stop\n",
    "from coffea.processor.spark.spark_executor import spark_executor\n",
    "from python.dimuon_processor import DimuonProcessor\n",
    "\n",
    "spark_config = pyspark.sql.SparkSession.builder \\\n",
    "    .appName('spark-executor-test-%s' % guid()) \\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .config('spark.executor.memory', '4g') \\\n",
    "    .config('spark.sql.execution.arrow.enabled','true') \\\n",
    "    .config('spark.sql.execution.arrow.maxRecordsPerBatch', 200000)\n",
    "\n",
    "spark = _spark_initialize(config=spark_config, log_level='WARN', \n",
    "                          spark_progress=False, laurelin_version='0.5.1')\n",
    "\n",
    "\n",
    "partitionsize = 200000\n",
    "thread_workers = 2\n",
    "\n",
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(samp_info.full_fileset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(samp_info.filesets_chunked.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the SPARK executor\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tstart = time.time()\n",
    "\n",
    "\"\"\"\n",
    "fileset1 = {\n",
    "    'DoubleMuon': { 'files': [\n",
    "        'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012B_DoubleMuParked.root',\n",
    "        'root://eospublic.cern.ch//eos/root-eos/cms_opendata_2012_nanoaod/Run2012C_DoubleMuParked.root',\n",
    "                             ], \n",
    "                    'treename': 'Events'\n",
    "                  }\n",
    "}\n",
    "output = processor.run_spark_job(fileset1, DimuonProcessor(samp_info=samp_info, do_jecunc=False), spark_executor, spark=spark,\\\n",
    "                                         partitionsize=partitionsize, thread_workers=thread_workers,\\\n",
    "                                         executor_args={'file_type': 'edu.vanderbilt.accre.laurelin.Root', 'cache': False, 'nano': True, 'retries': 5})\n",
    "\"\"\"\n",
    "output = processor.run_spark_job(samp_info.full_fileset, DimuonProcessor(samp_info=samp_info, do_jecunc=False), spark_executor, spark=spark,\\\n",
    "                                         partitionsize=partitionsize, thread_workers=thread_workers,\\\n",
    "                                         executor_args={'file_type': 'edu.vanderbilt.accre.laurelin.Root', 'cache': False, 'nano': True, 'retries': 5})\n",
    "\"\"\"\n",
    "for ds_name, fileset_ in samp_info.filesets_chunked.items():\n",
    "    for ichunk, ifileset in enumerate(fileset_):\n",
    "        print(f\"Processing {ds_name}, chunk {ichunk+1}/{samp_info.nchunks} ...\")\n",
    "        print(ifileset)\n",
    "\n",
    "#        output = processor.run_spark_job(fileset, DimuonProcessor(), spark_executor,\\\n",
    "#                                spark=spark, partitionsize=partitionsize, thread_workers=thread_workers,\\\n",
    "#                                executor_args={'file_type': 'edu.vanderbilt.accre.laurelin.Root', 'cache': False})\n",
    "        output = processor.run_spark_job(ifileset, DimuonProcessor(samp_info=samp_info, do_jecunc=False), spark_executor, spark=spark,\\\n",
    "                                         partitionsize=partitionsize, thread_workers=thread_workers,\\\n",
    "                                         executor_args={'file_type': 'edu.vanderbilt.accre.laurelin.Root', 'cache': False, 'nano': True, 'retries': 5})\n",
    "###\n",
    "        #out_dir = f\"/depot/cms/hmm/coffea/{samp_info.out_path}/\"\n",
    "        out_dir = f\"/depot/cms/hmm/coffea/Stefan/{samp_info.out_path}/\"\n",
    "        #out_dir = f\"/tmp/spiperov/hmm/coffea/{samp_info.out_path}/\"\n",
    "\n",
    "        try:\n",
    "            os.mkdir(out_dir)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for mode in output.keys():\n",
    "            out_dir_ = f\"{out_dir}/{mode}/\"\n",
    "            out_path_ = f\"{out_dir_}/{ds_name}_{ichunk}.coffea\"\n",
    "            try:\n",
    "                os.mkdir(out_dir_)\n",
    "            except:\n",
    "                pass\n",
    "            util.save(output[mode], out_path_)\n",
    "\n",
    "        output.clear()\n",
    "        print(f\"Saved output to {out_dir}\")\n",
    "    \n",
    "elapsed = time.time() - tstart\n",
    "\"\"\"\n",
    "print(f\"Total time: {elapsed} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Data/MC comparison\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob\n",
    "import argparse\n",
    "from python.postprocessing import postprocess, plot, save_shapes\n",
    "from config.variables import variables\n",
    "from config.datasets import datasets\n",
    "import pandas as pd\n",
    "\n",
    "year = '2016'\n",
    "label = 'test'\n",
    "\n",
    "to_plot = ['dimuon_mass']\n",
    "# to_plot = ['dimuon_mass','mu1_pt']\n",
    "vars_to_plot = {v.name:v for v in variables if v.name in to_plot}\n",
    "samples = [\n",
    "    'data_A',\n",
    "    'data_B',\n",
    "    'data_C',\n",
    "    'data_D',\n",
    "    'data_E',\n",
    "    'data_F',\n",
    "    'data_G',\n",
    "    'data_H',\n",
    "    'dy_m105_160_amc',\n",
    "    'dy_m105_160_vbf_amc',\n",
    "    'ewk_lljj_mll105_160_ptj0',\n",
    "    'ttjets_dl',\n",
    "    'ttjets_sl',\n",
    "    'ttz',\n",
    "    'ttw',\n",
    "    'st_tw_top','st_tw_antitop',\n",
    "    'ww_2l2nu',\n",
    "    'wz_2l2q',\n",
    "    'wz_3lnu',\n",
    "    'zz',\n",
    "    'ggh_amcPS',\n",
    "    'vbf_powhegPS',\n",
    "]\n",
    "\n",
    "\n",
    "postproc_args = {\n",
    "    'modules': ['to_pandas',  'get_hists'],\n",
    "    'year': year,\n",
    "    'label': label,\n",
    "#    'in_path': f'/depot/cms/hmm/coffea/test_{year}_{label}/',\n",
    "    'in_path': f'/depot/cms/hmm/coffea/Stefan/test_{year}_{label}/',\n",
    "#    'in_path': f'/tmp/spiperov/hmm/coffea/test_{year}_{label}/',\n",
    "    'syst_variations': ['nominal'],\n",
    "    'samples':samples,\n",
    "    'channels': ['vbf'],\n",
    "    'regions': ['h-peak', 'h-sidebands'],\n",
    "    'vars_to_plot': list(vars_to_plot.values()),\n",
    "    'wgt_variations': False,\n",
    "}\n",
    "\n",
    "\n",
    "dfs, hist_dfs, edges = postprocess(postproc_args)\n",
    "hist = {}\n",
    "for var, hists in hist_dfs.items():\n",
    "    hist[var] = pd.concat(hists, ignore_index=True)\n",
    "\n",
    "plot(vars_to_plot['dimuon_mass'], hist, 'wgt_nominal', edges['dimuon_mass'], postproc_args, save=False, show=True, plotsize=8)\n",
    "# for vname, var in vars_to_plot.items():\n",
    "#     for r in postproc_args['regions']:\n",
    "#        plot(var, hist, 'wgt_nominal', edges[vname], postproc_args, r, save=False, show=True, plotsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "sparkconnect": {
   "bundled_options": [
    "MemoryIntensive",
    "ComputeIntensive",
    "LongRunningAnalysis"
   ],
   "list_of_options": [
    {
     "name": "spark.kubernetes.container.image",
     "value": "gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin"
    },
    {
     "name": "spark.sql.execution.arrow.enabled",
     "value": "true"
    },
    {
     "name": "spark.sql.execution.arrow.maxRecordsPerBatch",
     "value": "200000"
    },
    {
     "name": "spark.kubernetes.container.image.pullPolicy",
     "value": "Always"
    },
    {
     "name": "spark.driver.extraClassPath",
     "value": "./laurelin-0.5.1.jar:./lz4-java-1.5.1.jar:./log4j-core-2.11.2.jar:./log4j-api-2.11.2.jar:./xz-1.2.jar"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
