{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img  src=\"https://swan.web.cern.ch/sites/swan.web.cern.ch/files/pictures/logo_swan_letters.png\" alt=\"SWAN\" style=\"float: left; width: 15%; margin-right: 5%; margin-left: 17%; margin-top: 1.0em; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://spark.apache.org/images/spark-logo-trademark.png\" alt=\"EP-SFT\" style=\"float: left; width: 25%; margin-right: 0%; margin-left: 0%; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://cms-docdb.cern.ch/cgi-bin/PublicDocDB/RetrieveFile?docid=3045&filename=CMSlogo_color_label_1024_May2014.png&version=3\" alt=\"CMS\" style=\"float: left; width: 12%; margin-left: 5%; margin-right: 5%; margin-bottom: 2.0em;\"> -->\n",
    "<p style=\"clear: both;\">\n",
    "<div style=\"text-align:center\"><h1>CMS H&#8594;µµ analysis  \n",
    "     <br> with Coffea package from Fermilab</h1></div>\n",
    "<div style=\"text-align:center\"><i>Author: Dmitry Kondratyev, based on example code by Lindsey Gray<br> and adapted for the demonstration by Stefan Piperov</i></div>\n",
    "<hr style=\"border-top-width: 4px; border-top-color: #34609b;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for Higgs boson decaying into two muons\n",
    "\n",
    "This code uses awkward array toolset, and utilizing Coffea [histograms](https://coffeateam.github.io/coffea/modules/coffea.hist.html).\n",
    "This also shows the analysis object syntax implemented by Coffea [JaggedCandidateArray](https://coffeateam.github.io/coffea/api/coffea.analysis_objects.JaggedCandidateMethods.html), and the usage of custom [accumulators](https://coffeateam.github.io/coffea/api/coffea.processor.AccumulatorABC.html) other than histograms.  Further, it introduces the [processor](https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html) concept and the interface to apache spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions to run on Hammer at Purdue:\n",
    "===\n",
    "It is assumed that this demonstration is being run in a Jupyter Lab session on Hammer, as described in the README file.  \n",
    "  \n",
    "**NOTE** You need a separate terminal from which to create and control your DASK cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -l\n",
    "hostname\n",
    "pwd\n",
    "ml anaconda/5.3.1-py37\n",
    "source activate hmumu_coffea\n",
    "voms-proxy-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare directories for output files  \n",
    "\n",
    "**Note** these paths need to be changed also further down in \"Out_dir\" in the \"Run the DASK Executor\" cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hammer-c026\n",
      "/home/spiperov/dmitry/hmumu-coffea_01May2020\n",
      "subject   : /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=piperov/CN=422973/CN=Stefcho Piperov/CN=951169794\n",
      "issuer    : /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=piperov/CN=422973/CN=Stefcho Piperov\n",
      "identity  : /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=piperov/CN=422973/CN=Stefcho Piperov\n",
      "type      : RFC compliant proxy\n",
      "strength  : 1024 bits\n",
      "path      : /home/spiperov/x509up_u638764\n",
      "timeleft  : 191:21:43\n"
     ]
    }
   ],
   "source": [
    "mkdir -p /depot/cms/hmm/coffea/Stefan/test_2018_test/unbinned\n",
    "mkdir -p /depot/cms/hmm/coffea/Stefan/test_2018_test/binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import coffea\n",
    "print(\"Coffea version: \", coffea.__version__)\n",
    "import socket\n",
    "\n",
    "print(socket.gethostname())\n",
    "from coffea import util\n",
    "import coffea.processor as processor\n",
    "import multiprocessing as mp\n",
    "print(f\"{mp.cpu_count()} CPUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data samples\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.samples_info import SamplesInfo\n",
    "samples = [\n",
    "### Data ###\n",
    "    'data_A', #not available for 2016\n",
    "    'data_B',\n",
    "    'data_C',\n",
    "    'data_D','data_E',\n",
    "    'data_F',\n",
    "    'data_G','data_H',\n",
    "\n",
    "### Essential MC ###    \n",
    "    'dy_m105_160_amc', \n",
    "    'dy_m105_160_vbf_amc',\n",
    "#      'ggh_amcPS', \n",
    "#     'vbf_powhegPS', \n",
    "#     'ttjets_dl',\n",
    "#    \"ewk_lljj_mll105_160_ptj0\",\n",
    "\n",
    "### Non-essential MC ### \n",
    "#     'ttjets_sl',\n",
    "#     'ttz',\n",
    "#     'ttw',\n",
    "#     'st_tw_top','st_tw_antitop',\n",
    "#     'ww_2l2nu',\n",
    "#     'wz_2l2q',\n",
    "#     'wz_3lnu',\n",
    "#     'wz_1l1nu2q',\n",
    "#      'zz',\n",
    "# # ##\n",
    "    \n",
    "]\n",
    "\n",
    "purdue = 'root://xrootd.rcac.purdue.edu/'\n",
    "\n",
    "year = '2018'\n",
    "label = 'test' # change this to save to other directory\n",
    "\n",
    "samp_info = SamplesInfo(year=year, out_path=f'test_{year}_{label}', server=purdue, debug=True)\n",
    "\n",
    "# 'outer' refers to parallelization by sample, 'inner' - by ROOT file in each sample\n",
    "samp_info.load(samples, nchunks=1, parallelize_outer=1, parallelize_inner=10)\n",
    "samp_info.compute_lumi_weights()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Dask scheduler\n",
    "===\n",
    "**Note** that in the next cell, in the line  \n",
    "`client = distributed.Client('128.211.149.132:36442')`  \n",
    "you need to substitute the IP address and TCP port number of your previously started DASK scheduler (e.g. as reported by the \"print(cluster)\" command in the scheduler terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import pytest\n",
    "from coffea.processor.executor import dask_executor\n",
    "import dask\n",
    "from python.dimuon_processor import DimuonProcessor\n",
    "\n",
    "try_cluster = True\n",
    "\n",
    "if try_cluster:\n",
    "    from dask_jobqueue import SLURMCluster\n",
    "    distributed = pytest.importorskip(\"distributed\", minversion=\"1.28.1\")\n",
    "    distributed.config['distributed']['worker']['memory']['target'] = 0.75\n",
    "    distributed.config['distributed']['worker']['memory']['spill'] = 0.80\n",
    "    distributed.config['distributed']['worker']['memory']['pause'] = 0.95\n",
    "    distributed.config['distributed']['worker']['memory']['terminate'] = False\n",
    "#    distributed.config['distributed']['worker']['memory']['terminate'] = 1.50\n",
    "    distributed.config['distributed']['worker']['connections']['outgoing'] = 500\n",
    "    distributed.config['distributed']['worker']['connections']['incoming'] = 100\n",
    "    distributed.config['distributed']['scheduler']['allowed-failures'] = 5\n",
    "    distributed.config['distributed']['scheduler']['bandwidth'] = 1000000000\n",
    "    distributed.config['distributed']['scheduler']['work-stealing'] = False\n",
    "    distributed.config['distributed']['scheduler']['default-task-durations'] = \"1h\"\n",
    "    distributed.config['distributed']['comm']['retry']['count'] = 2\n",
    "    distributed.config['distributed']['comm']['socket-backlog'] = 20480\n",
    "    distributed.config['distributed']['comm']['offload'] = \"100MiB\"\n",
    "    distributed.config['distributed']['deploy']['lost-worker-timeout'] = \"1m\"\n",
    "    \n",
    "    client = distributed.Client('128.211.149.132:36442')\n",
    "    \n",
    "else:\n",
    "    n_workers = 46\n",
    "    distributed = pytest.importorskip(\"distributed\", minversion=\"1.28.1\")\n",
    "    distributed.config['distributed']['worker']['memory']['terminate'] = False\n",
    "    client = distributed.Client(processes=True, dashboard_address=None, n_workers=n_workers, threads_per_worker=1, memory_limit='12GB') \n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client.scheduler_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client.restart()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Dask executor\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tstart = time.time()\n",
    "print(client)\n",
    "\n",
    "for ds_name, fileset_ in samp_info.filesets_chunked.items():\n",
    "#    client.restart()\n",
    "    for ichunk, ifileset in enumerate(fileset_):\n",
    "        print(f\"Processing {ds_name}, chunk {ichunk+1}/{samp_info.nchunks} ...\")\n",
    "        output = processor.run_uproot_job(ifileset, 'Events',\\\n",
    "                                      DimuonProcessor(samp_info=samp_info, do_jecunc=False),\\\n",
    "                                      dask_executor,\\\n",
    "                                      executor_args={'nano': True, 'client': client})\n",
    "\n",
    "        #out_dir = f\"/depot/cms/hmm/coffea/{samp_info.out_path}/\"\n",
    "        out_dir = f\"/depot/cms/hmm/coffea/Stefan/{samp_info.out_path}/\"\n",
    "        #out_dir = f\"/tmp/spiperov/hmm/coffea/{samp_info.out_path}/\"\n",
    "\n",
    "        try:\n",
    "            os.mkdir(out_dir)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for mode in output.keys():\n",
    "            out_dir_ = f\"{out_dir}/{mode}/\"\n",
    "            out_path_ = f\"{out_dir_}/{ds_name}_{ichunk}.coffea\"\n",
    "            try:\n",
    "                os.mkdir(out_dir_)\n",
    "            except:\n",
    "                pass\n",
    "            util.save(output[mode], out_path_)\n",
    "\n",
    "        output.clear()\n",
    "        print(f\"Saved output to {out_dir}\")\n",
    "    \n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Total time: {elapsed} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Data/MC comparison\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob\n",
    "import argparse\n",
    "from python.postprocessing import postprocess, plot, save_shapes\n",
    "from config.variables import variables\n",
    "from config.datasets import datasets\n",
    "import pandas as pd\n",
    "\n",
    "year = '2018'\n",
    "label = 'test'\n",
    "\n",
    "to_plot = ['dimuon_mass']\n",
    "# to_plot = ['dimuon_mass','mu1_pt']\n",
    "vars_to_plot = {v.name:v for v in variables if v.name in to_plot}\n",
    "samples = [\n",
    "    'data_A',\n",
    "    'data_B',\n",
    "    'data_C',\n",
    "    'data_D',\n",
    "    'data_E',\n",
    "    'data_F',\n",
    "    'data_G',\n",
    "    'data_H',\n",
    "    'dy_m105_160_amc',\n",
    "    'dy_m105_160_vbf_amc',\n",
    "    'ewk_lljj_mll105_160_ptj0',\n",
    "    'ttjets_dl',\n",
    "    'ttjets_sl',\n",
    "    'ttz',\n",
    "    'ttw',\n",
    "    'st_tw_top','st_tw_antitop',\n",
    "    'ww_2l2nu',\n",
    "    'wz_2l2q',\n",
    "    'wz_3lnu',\n",
    "    'zz',\n",
    "    'ggh_amcPS',\n",
    "    'vbf_powhegPS',\n",
    "]\n",
    "\n",
    "\n",
    "postproc_args = {\n",
    "    'modules': ['to_pandas',  'get_hists'],\n",
    "    'year': year,\n",
    "    'label': label,\n",
    "#    'in_path': f'/depot/cms/hmm/coffea/test_{year}_{label}/',\n",
    "    'in_path': f'/depot/cms/hmm/coffea/Stefan/test_{year}_{label}/',\n",
    "#    'in_path': f'/tmp/spiperov/hmm/coffea/test_{year}_{label}/',\n",
    "    'syst_variations': ['nominal'],\n",
    "    'samples':samples,\n",
    "    'channels': ['vbf'],\n",
    "    'regions': ['h-peak', 'h-sidebands'],\n",
    "    'vars_to_plot': list(vars_to_plot.values()),\n",
    "    'wgt_variations': False,\n",
    "}\n",
    "\n",
    "\n",
    "dfs, hist_dfs, edges = postprocess(postproc_args)\n",
    "hist = {}\n",
    "for var, hists in hist_dfs.items():\n",
    "    hist[var] = pd.concat(hists, ignore_index=True)\n",
    "\n",
    "plot(vars_to_plot['dimuon_mass'], hist, 'wgt_nominal', edges['dimuon_mass'], postproc_args, save=False, show=True, plotsize=8)\n",
    "# for vname, var in vars_to_plot.items():\n",
    "#     for r in postproc_args['regions']:\n",
    "#        plot(var, hist, 'wgt_nominal', edges[vname], postproc_args, r, save=False, show=True, plotsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hmumu_coffea] *",
   "language": "python",
   "name": "conda-env-hmumu_coffea-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "sparkconnect": {
   "bundled_options": [
    "MemoryIntensive",
    "ComputeIntensive",
    "LongRunningAnalysis"
   ],
   "list_of_options": [
    {
     "name": "spark.kubernetes.container.image",
     "value": "gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin"
    },
    {
     "name": "spark.sql.execution.arrow.enabled",
     "value": "true"
    },
    {
     "name": "spark.sql.execution.arrow.maxRecordsPerBatch",
     "value": "200000"
    },
    {
     "name": "spark.kubernetes.container.image.pullPolicy",
     "value": "Always"
    },
    {
     "name": "spark.driver.extraClassPath",
     "value": "./laurelin-0.5.1.jar:./lz4-java-1.5.1.jar:./log4j-core-2.11.2.jar:./log4j-api-2.11.2.jar:./xz-1.2.jar"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
